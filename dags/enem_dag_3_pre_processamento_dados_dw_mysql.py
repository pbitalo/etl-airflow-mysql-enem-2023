from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from airflow.models import Variable
import pandas as pd
import os
import time

# DefiniÃ§Ã£o dos argumentos da DAG
default_args = {
    "owner": "airflow",
    'start_date': datetime.now(),
    "catchup": False
}

# DefiniÃ§Ã£o do tamanho do chunk e limite de processamento
CHUNK_TAMANHO = int(Variable.get("CHUNK_SIZE"))
CHUNK_QTD = int(Variable.get("CHUNK_PROCESS_QTD"))

# Caminhos dos arquivos
ARQUIVO_ENTRADA = "/opt/airflow/data/enem_2023/DADOS/MICRODADOS_ENEM_2023.csv"
ARQUIVO_SAIDA = "/opt/airflow/staging/enem_2023_cleaned.csv"

def verificar_arquivo():
    """Verifica se o arquivo de entrada existe antes de iniciar o processamento."""
    if not os.path.exists(ARQUIVO_ENTRADA):
        print(f"âŒ Erro: O arquivo {ARQUIVO_ENTRADA} nÃ£o foi encontrado!\nCertifique-se de que a DAG de download e descompactaÃ§Ã£o rodou corretamente.")
        time.sleep(3)  
        raise FileNotFoundError(f"âŒ Erro: O arquivo {ARQUIVO_ENTRADA} nÃ£o foi encontrado!\nCertifique-se de que a DAG de download e descompactaÃ§Ã£o rodou corretamente.")

    print(f"âœ… Arquivo encontrado: {ARQUIVO_ENTRADA}")

def carregar_pre_processar_dados():
    """Processa e limpa os dados do ENEM 2023, salvando em um arquivo formatado."""

    # ðŸ”¹ A verificaÃ§Ã£o do arquivo jÃ¡ foi feita na DAG, entÃ£o aqui sÃ³ processamos os dados

    # Garante que o diretÃ³rio de saÃ­da existe
    os.makedirs("/opt/airflow/staging", exist_ok=True)

    chunk_idx = 0  # Contador de chunks processados
    total_chunks = sum(1 for _ in pd.read_csv(ARQUIVO_ENTRADA, delimiter=";", encoding="latin-1", chunksize=CHUNK_TAMANHO))  # Conta chunks totais

    # LÃª o arquivo em chunks e processa um nÃºmero limitado de chunks
    with pd.read_csv(ARQUIVO_ENTRADA, delimiter=';', encoding="latin-1", chunksize=CHUNK_TAMANHO) as reader:
        for i, chunk in enumerate(reader):
            
            print(f"ðŸ“‚ Processamento TOTAL da Base => Chunk {i + 1}...")

            chunk_idx += 1
            progresso = (chunk_idx / total_chunks) * 100
            
            print(f"âœ… Processamento TOTAL da Base => Chunk {chunk_idx}/{total_chunks} ({progresso:.2f}%) - {len(chunk)} registros...")      
                  
            if chunk_idx > CHUNK_QTD:
                print(f"ðŸ”¹ Limite de {CHUNK_QTD} chunks atingido. Parando processamento.")
                break  # Para o loop apÃ³s atingir o limite

            # ðŸ”¹ Seleciona apenas as colunas necessÃ¡rias
            cols = [
                'TP_FAIXA_ETARIA',  # Faixa etÃ¡ria do candidato
                'SG_UF_PROVA',       # Estado onde fez a prova
                'NU_NOTA_MT',        # Nota MatemÃ¡tica
                'NU_NOTA_CN',        # Nota CiÃªncias da Natureza
                'NU_NOTA_LC',        # Nota Linguagens e CÃ³digos
                'NU_NOTA_CH',        # Nota CiÃªncias Humanas
                'NU_NOTA_REDACAO',   # Nota RedaÃ§Ã£o
                'TP_SEXO'            # Sexo do candidato
            ]
            
            # MantÃ©m apenas as colunas necessÃ¡rias
            chunk = chunk[cols]

            # ðŸ”¹ Trata valores nulos
            chunk = chunk.fillna({
                'TP_SEXO': 'N/I',   # Define 'N/I' para sexo nÃ£o informado
                'TP_FAIXA_ETARIA': -1,  # Define -1 para faixa etÃ¡ria nÃ£o informada
                'NU_NOTA_MT': 0,    # Substitui notas ausentes por 0
                'NU_NOTA_CN': 0,
                'NU_NOTA_LC': 0,
                'NU_NOTA_CH': 0,
                'NU_NOTA_REDACAO': 0
            })

            # ðŸ”¹ Converte tipos de dados
            chunk['TP_SEXO'] = chunk['TP_SEXO'].astype(str)
            chunk['TP_FAIXA_ETARIA'] = chunk['TP_FAIXA_ETARIA'].astype(int)

            # ðŸ”¹ Salva o chunk processado no arquivo de saÃ­da
            chunk.to_csv(
                ARQUIVO_SAIDA, 
                index=False, 
                mode='w' if chunk_idx == 1 else 'a',  # 'w' para o primeiro chunk, 'a' para os seguintes
                header=True if chunk_idx == 1 else False  # Apenas o primeiro chunk deve ter cabeÃ§alho
            )
            
            print(f"âœ… Chunk {chunk_idx} salvo com sucesso.")

        print("ðŸš€ Processamento finalizado!")

# Criando a DAG
dag = DAG(
    "etl_enem_2023_p3_pre_processamento_dados_dw_mysql",
    default_args=default_args,
    schedule_interval="@once"
)

# Criando a tarefa de verificaÃ§Ã£o do arquivo antes do processamento
tarefa_verificar_arquivo_existe = PythonOperator(
    task_id="verificar_arquivo",
    python_callable=verificar_arquivo,
    dag=dag
)

# Criando a tarefa de carga e limpeza
tarefa_pre_processamento = PythonOperator(
    task_id="carregar_pre_processar_dados",
    python_callable=carregar_pre_processar_dados,
    dag=dag
)

tarefa_verificar_arquivo_existe >> tarefa_pre_processamento

